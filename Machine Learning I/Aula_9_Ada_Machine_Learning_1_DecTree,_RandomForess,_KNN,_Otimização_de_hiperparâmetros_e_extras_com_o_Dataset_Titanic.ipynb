{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstração do conceito de separabilidade linear vs não linear"
      ],
      "metadata": {
        "id": "p2GitgzrHDXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Criar dados de exemplo com duas features\n",
        "np.random.seed(42)\n",
        "X = np.random.randn(100, 2)\n",
        "y = np.zeros(100)\n",
        "y[X[:, 0] + X[:, 1] > 0] = 1  # Classificação linear\n",
        "\n",
        "# Treinar um classificador linear (Regressão Logística)\n",
        "clf_linear = LogisticRegression()\n",
        "clf_linear.fit(X, y)\n",
        "\n",
        "# Treinar um classificador baseado em árvore de decisão\n",
        "clf_tree = DecisionTreeClassifier()\n",
        "clf_tree.fit(X, y)\n",
        "\n",
        "# Plotar os dados e as fronteiras de decisão dos classificadores\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
        "plt.title('Fronteira de Decisão Linear')\n",
        "ax = plt.gca()\n",
        "xlim = ax.get_xlim()\n",
        "ylim = ax.get_ylim()\n",
        "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50),\n",
        "                     np.linspace(ylim[0], ylim[1], 50))\n",
        "Z = clf_linear.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.4, cmap='coolwarm')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
        "plt.title('Fronteira de Decisão de Árvore de Decisão')\n",
        "ax = plt.gca()\n",
        "xlim = ax.get_xlim()\n",
        "ylim = ax.get_ylim()\n",
        "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50),\n",
        "                     np.linspace(ylim[0], ylim[1], 50))\n",
        "Z = clf_tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.4, cmap='coolwarm')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3ATad32uHDo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pré-processamento - Transformação dos datasets de treinamento e teste"
      ],
      "metadata": {
        "id": "SL62w4hFFEWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação de pacotes"
      ],
      "metadata": {
        "id": "poCiX600x6gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "id": "lnCqexNsyBiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregamento de dataset, EDA básica e exclusão de colunas indesejadas"
      ],
      "metadata": {
        "id": "0XjUbfatyCMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o conjunto de dados Titanic\n",
        "titanic_data = pd.read_csv('train.csv')\n",
        "\n",
        "# Vamos checar a qtd de valores únicos e a proporção de nulos por coluna\n",
        "for col in titanic_data.columns:\n",
        "  qtd_unique = len(titanic_data[col].unique())\n",
        "  perc_nulls = titanic_data[col].isna().mean()\n",
        "  print(f\"{col}: {qtd_unique} unique values, and {100*perc_nulls:.2f}% of nulls\")\n",
        "\n",
        "# Vamos remover algumas colunas que não serão úteis para este exemplo\n",
        "titanic_data = titanic_data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j-uKY-iyg9K",
        "outputId": "f28aa95e-173c-42ef-f7f8-764020087ada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PassengerId: 891 unique values, and 0.00% of nulls\n",
            "Survived: 2 unique values, and 0.00% of nulls\n",
            "Pclass: 3 unique values, and 0.00% of nulls\n",
            "Name: 891 unique values, and 0.00% of nulls\n",
            "Sex: 2 unique values, and 0.00% of nulls\n",
            "Age: 89 unique values, and 19.87% of nulls\n",
            "SibSp: 7 unique values, and 0.00% of nulls\n",
            "Parch: 7 unique values, and 0.00% of nulls\n",
            "Ticket: 681 unique values, and 0.00% of nulls\n",
            "Fare: 248 unique values, and 0.00% of nulls\n",
            "Cabin: 148 unique values, and 77.10% of nulls\n",
            "Embarked: 4 unique values, and 0.22% of nulls\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformação dos datasets de treinamento e teste"
      ],
      "metadata": {
        "id": "H42CBsVzzEzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos definir uma função que encapsulará as nossas rotinas de transformação de datasets\n",
        "def transform_dataset(df, num_imputer, cat_imputer, onehot_encoder, num_variables, cat_variables):\n",
        "\n",
        "  # Aplicação do numerical imputer\n",
        "  df[num_variables] = num_imputer.transform(df[num_variables])\n",
        "\n",
        "  # Aplicação do categorical imputer\n",
        "  df[cat_variables] = cat_imputer.transform(df[cat_variables])\n",
        "\n",
        "  # Aplicação do one_hot encoder\n",
        "  dummy_variables_df = pd.DataFrame(\n",
        "    data = onehot_encoder.transform(df[cat_variables]).toarray(),\n",
        "    columns = onehot_encoder.get_feature_names_out(),\n",
        "    index = df.index\n",
        "  )\n",
        "  df = pd.concat([\n",
        "    df.drop(columns = cat_variables),\n",
        "    dummy_variables_df\n",
        "  ], axis = 1)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "BpdbE3_61u74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3BxagzIEAdH"
      },
      "outputs": [],
      "source": [
        "# Definindo colunas numericas\n",
        "numerical_original_columns = ['Age', 'Fare']\n",
        "\n",
        "# Definindo colunas categóricas\n",
        "categorical_original_columns = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n",
        "\n",
        "# Separando as features e o target\n",
        "X = titanic_data.drop(columns = ['Survived'])\n",
        "y = titanic_data['Survived']\n",
        "\n",
        "# Dividir os dados em conjunto de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Ajustando imputers para valores numéricos ausentes com base no conjunto de treinamento\n",
        "numerical_imputer = SimpleImputer(strategy='mean') # imputaremos os nulos com a média da coluna\n",
        "numerical_imputer.fit(X_train[numerical_original_columns])\n",
        "\n",
        "# Ajustando imputers para valores categóricos ausentes com base no conjunto de treinamento\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')  # imputaremos os nulos com a moda da coluna\n",
        "categorical_imputer.fit(X_train[categorical_original_columns])\n",
        "\n",
        "# Ajustando um transformador de varáveis categóricas com base no conjunto de treinamento\n",
        "one_hot_encoder = OneHotEncoder()\n",
        "one_hot_encoder.fit(X_train[categorical_original_columns])\n",
        "\n",
        "# Transformando o dataset de treinamento de acordo com os objetos ajustados anteriormente\n",
        "X_train = transform_dataset(\n",
        "  X_train, numerical_imputer, categorical_imputer, one_hot_encoder,\n",
        "  numerical_original_columns, categorical_original_columns\n",
        ")\n",
        "\n",
        "# Transformando o dataset de teste de acordo com os objetos ajustados em treinamento\n",
        "X_test = transform_dataset(\n",
        "  X_test, numerical_imputer, categorical_imputer, one_hot_encoder,\n",
        "  numerical_original_columns, categorical_original_columns\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Classifier com hiperparâmetros default"
      ],
      "metadata": {
        "id": "De0delofH1oW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinando o modelo decision_tree\n",
        "dec_tree_def_clf = DecisionTreeClassifier()\n",
        "dec_tree_def_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predizendo saídas para o dataset de treinamento\n",
        "y_train_pred = dec_tree_def_clf.predict(X_train)\n",
        "\n",
        "# Predizendo saídas para o dataset de testes\n",
        "y_test_pred = dec_tree_def_clf.predict(X_test)\n",
        "\n",
        "# Calculando AUC em treinamento e em teste\n",
        "roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
        "roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"AUC_train = {roc_auc_train}, AUC_tes = {roc_auc_test}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3r-8_ro3PSU",
        "outputId": "1dec659c-1382-42db-b867-02d8175f5264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC_train = 0.9746201425305903, AUC_tes = 0.7696267696267697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Classifier com hiperparâmetros otimizados via grid-search"
      ],
      "metadata": {
        "id": "9V0CaG9b7Pcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Definindo o grid de parâmetros a serem testados\n",
        "param_grid = {\n",
        "  'criterion': ['gini', 'entropy'],\n",
        "  'max_depth': [None, 5, 10, 15],\n",
        "  'min_samples_split': [2, 5, 10],\n",
        "  'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "  estimator = DecisionTreeClassifier(),\n",
        "  param_grid = param_grid,\n",
        "  cv = 5\n",
        ")\n",
        "\n",
        "# Ajustando o GridSearch ao conjunto de treinamento\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Obtendo os melhores parâmetros encontrados\n",
        "dec_tree_gs_best_params = grid_search.best_params_\n",
        "print(dec_tree_gs_best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgFyogVZ7MuF",
        "outputId": "633e15af-96e8-4946-f00e-aba9a4f1c872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinando o modelo\n",
        "dec_tree_gs_clf = DecisionTreeClassifier(**dec_tree_gs_best_params)\n",
        "dec_tree_gs_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predizendo saídas para o dataset de treinamento\n",
        "y_train_pred = dec_tree_gs_clf.predict(X_train)\n",
        "\n",
        "# Predizendo saídas para o dataset de testes\n",
        "y_test_pred = dec_tree_gs_clf.predict(X_test)\n",
        "\n",
        "# Calculando AUC em treinamento e em teste\n",
        "roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
        "roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"AUC_train = {roc_auc_train}, AUC_tes = {roc_auc_test}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilf_dJBo7wJp",
        "outputId": "e5a3c03b-6623-4b84-faf2-849408fb32b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC_train = 0.8021379588543768, AUC_tes = 0.7834620334620336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Classifier com hiperparâmetros otimizados via random-search"
      ],
      "metadata": {
        "id": "mSOV6frS8gYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Definindo o espaço de busca para os hiperparâmetros\n",
        "param_dist = {\n",
        "  'criterion': ['gini', 'entropy'],\n",
        "  'max_depth': randint(1, 20),\n",
        "  'min_samples_split': randint(2, 20),\n",
        "  'min_samples_leaf': randint(1, 10)\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "  estimator = DecisionTreeClassifier(),\n",
        "  param_distributions = param_dist,\n",
        "  n_iter = 200,\n",
        "  cv = 5\n",
        ")\n",
        "\n",
        "# Ajustando a pesquisa aleatória ao conjunto de treinamento\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Obtendo os melhores parâmetros encontrados\n",
        "dec_tree_rs_best_params = random_search.best_params_\n",
        "print(dec_tree_rs_best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIkIiQV38rSN",
        "outputId": "9249e7e1-b552-41d1-af37-34ca01dc6973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinando o modelo\n",
        "dec_tree_rs_clf = DecisionTreeClassifier(**dec_tree_rs_best_params)\n",
        "dec_tree_rs_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predizendo saídas para o dataset de treinamento\n",
        "y_train_pred = dec_tree_rs_clf.predict(X_train)\n",
        "\n",
        "# Predizendo saídas para o dataset de testes\n",
        "y_test_pred = dec_tree_rs_clf.predict(X_test)\n",
        "\n",
        "# Calculando AUC em treinamento e em teste\n",
        "roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
        "roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"AUC_train = {roc_auc_train}, AUC_tes = {roc_auc_test}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLdbJTVk9GrH",
        "outputId": "1ecd4a35-6a98-47c2-bd9e-1913c92a0eb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC_train = 0.8101384967056608, AUC_tes = 0.7779279279279279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Classifier com hiperparâmetros otimizados via random-search"
      ],
      "metadata": {
        "id": "C1RfHfHQFKXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Definindo o espaço de busca para os hiperparâmetros\n",
        "param_dist = {\n",
        "    'n_estimators': randint(10, 200),\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None] + list(randint(1, 20).rvs(5)),\n",
        "    'min_samples_split': randint(2, 20),\n",
        "    'min_samples_leaf': randint(1, 10),\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "  estimator=RandomForestClassifier(),\n",
        "  param_distributions=param_dist,\n",
        "  n_iter=200,\n",
        "  cv=5,\n",
        "  random_state=42\n",
        ")\n",
        "\n",
        "# Ajustando a pesquisa aleatória ao conjunto de treinamento\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Obtendo os melhores parâmetros encontrados\n",
        "rforest_rs_best_params = random_search.best_params_\n",
        "print(rforest_rs_best_params)"
      ],
      "metadata": {
        "id": "v0Qev4jLEDCT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad904179-29ec-43ad-85cc-f92181eedf82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'criterion': 'entropy', 'max_depth': 5, 'max_features': None, 'min_samples_leaf': 6, 'min_samples_split': 3, 'n_estimators': 136}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinando o modelo\n",
        "rforest_rs_clf = RandomForestClassifier(**rforest_rs_best_params)\n",
        "rforest_rs_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predizendo saídas para o dataset de treinamento\n",
        "y_train_pred = rforest_rs_clf.predict(X_train)\n",
        "\n",
        "# Predizendo saídas para o dataset de testes\n",
        "y_test_pred = rforest_rs_clf.predict(X_test)\n",
        "\n",
        "# Calculando AUC em treinamento e em teste\n",
        "roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
        "roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"AUC_train = {roc_auc_train}, AUC_tes = {roc_auc_test}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUQdq21vBXSm",
        "outputId": "29f2ac78-34c5-4013-cc73-8e06573631c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC_train = 0.8214669893774371, AUC_tes = 0.7671814671814672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN com hiperparâmetros otimizados via random-search"
      ],
      "metadata": {
        "id": "Z2lT_c4uLbcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Definindo o espaço de busca para os hiperparâmetros\n",
        "param_dist = {\n",
        "    'n_neighbors': randint(1, 50),\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'p': [1, 2]  # para a distância de Minkowski (1 para Manhattan, 2 para Euclidiana)\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "  estimator = KNeighborsClassifier(),\n",
        "  param_distributions = param_dist,\n",
        "  n_iter = 200,\n",
        "  cv = 5\n",
        ")\n",
        "\n",
        "# Ajustando a pesquisa aleatória ao conjunto de treinamento\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Obtendo os melhores parâmetros encontrados\n",
        "knn_rs_best_params = random_search.best_params_\n",
        "print(knn_rs_best_params)"
      ],
      "metadata": {
        "id": "Y4oZCTYzLcQ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ec8ffbd-30ef-4249-e8ea-12159e3eb7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_neighbors': 15, 'p': 1, 'weights': 'distance'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinando o modelo\n",
        "knn_rs_clf = KNeighborsClassifier(**knn_rs_best_params)\n",
        "knn_rs_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predizendo saídas para o dataset de treinamento\n",
        "y_train_pred = knn_rs_clf.predict(X_train)\n",
        "\n",
        "# Predizendo saídas para o dataset de testes\n",
        "y_test_pred = knn_rs_clf.predict(X_test)\n",
        "\n",
        "# Calculando AUC em treinamento e em teste\n",
        "roc_auc_train = roc_auc_score(y_train, y_train_pred)\n",
        "roc_auc_test = roc_auc_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"AUC_train = {roc_auc_train}, AUC_tes = {roc_auc_test}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecu7fzGZQqAl",
        "outputId": "25a17b9a-264e-48e9-e85b-81817b06bd2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC_train = 0.9746201425305903, AUC_tes = 0.7393822393822393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-validation com Pipelines"
      ],
      "metadata": {
        "id": "DURWfklWa91E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Definindo colunas numericas\n",
        "numerical_original_columns = ['Age', 'Fare']\n",
        "\n",
        "# Definindo colunas categóricas\n",
        "categorical_original_columns = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n",
        "\n",
        "# Separando as features e o target\n",
        "X = titanic_data.drop(columns=['Survived'])\n",
        "y = titanic_data['Survived']\n",
        "\n",
        "# Dividir os dados em conjunto de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Criando pipeline para pré-processamento e classificação\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', ColumnTransformer([\n",
        "        ('numeric', SimpleImputer(strategy='mean'), numerical_original_columns),\n",
        "        ('categorical', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('onehot', OneHotEncoder(categories='auto', handle_unknown='ignore'))\n",
        "        ]), categorical_original_columns)\n",
        "    ])),\n",
        "    ('classifier', DecisionTreeClassifier())\n",
        "])\n",
        "\n",
        "# Treinando o modelo e avaliando usando cross-validation\n",
        "scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Ajustando o modelo final usando todos os dados de treinamento\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Avaliando o modelo no conjunto de teste\n",
        "roc_auc_test = roc_auc_score(y_test, pipeline.predict(X_test))\n",
        "\n",
        "print(f\"AUC_train (CV) = {scores.mean()}, AUC_test = {roc_auc_test}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CYC6wF6RCfz",
        "outputId": "91a5caef-c21a-4964-c812-6317e0a4d506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC_train (CV) = 0.7584457795725401, AUC_test = 0.7513513513513514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-validation com Pipelines incluindo random search"
      ],
      "metadata": {
        "id": "GAO8g695c_Ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from scipy.stats import randint\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pandas as pd\n",
        "\n",
        "# Definindo colunas numericas\n",
        "numerical_original_columns = ['Age', 'Fare']\n",
        "\n",
        "# Definindo colunas categóricas\n",
        "categorical_original_columns = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n",
        "\n",
        "# Separando as features e o target\n",
        "X = titanic_data.drop(columns=['Survived'])\n",
        "y = titanic_data['Survived']\n",
        "\n",
        "# Dividir os dados em conjunto de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Criando pipeline para pré-processamento e classificação\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', ColumnTransformer([\n",
        "        ('numeric', SimpleImputer(strategy='mean'), numerical_original_columns),\n",
        "        ('categorical', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('onehot', OneHotEncoder(categories='auto', handle_unknown='ignore'))\n",
        "        ]), categorical_original_columns)\n",
        "    ])),\n",
        "    ('classifier', DecisionTreeClassifier())\n",
        "])\n",
        "\n",
        "# Definindo o espaço de busca de hiperparâmetros\n",
        "param_dist = {\n",
        "    'classifier__max_depth': randint(1, 20),  # Profundidade máxima da árvore\n",
        "    'classifier__min_samples_split': randint(2, 20),  # Número mínimo de amostras necessárias para dividir um nó interno\n",
        "    'classifier__min_samples_leaf': randint(1, 20),  # Número mínimo de amostras necessárias para estar em um nó folha\n",
        "}\n",
        "\n",
        "# Criando o objeto RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=50, cv=5, scoring='accuracy')\n",
        "\n",
        "# Executando a busca aleatória de hiperparâmetros\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Melhores hiperparâmetros encontrados\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "# Melhor estimador encontrado\n",
        "best_estimator = random_search.best_estimator_\n",
        "\n",
        "# Avaliando o modelo no conjunto de teste usando os melhores hiperparâmetros encontrados\n",
        "roc_auc_test = roc_auc_score(y_test, best_estimator.predict(X_test))\n",
        "\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "print(f\"AUC_train (CV) = {random_search.best_score_}, AUC_test = {roc_auc_test}\")\n"
      ],
      "metadata": {
        "id": "pc2ggswjd9f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc43bfc5-168a-40ee-df1e-1dd79e9510c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'classifier__max_depth': 9, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 16}\n",
            "AUC_train (CV) = 0.8159854230276766, AUC_test = 0.8084942084942084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vjDrTZ531s0Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
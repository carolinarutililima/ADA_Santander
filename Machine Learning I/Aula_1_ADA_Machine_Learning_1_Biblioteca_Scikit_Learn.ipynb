{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Biblioteca Scikit-Learn"
      ],
      "metadata": {
        "id": "irn4wj1kCR2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conceito\n",
        "\n",
        "O scikit-learn é uma biblioteca criada para facilitar atividades de dados na área de _machine learning_. É uma biblioteca estruturada na linguagem python, _open source_ e comercialmente utilizada com licença Berkeley Source Distribution (BSD) que são um tipo de licença de baixa restrição para _software_ de código aberto que não impõe requisitos de redistribuição.\n",
        "\n",
        "Esta biblioteca oferece uma gama de ferramentas para gerar análises preditivas de dados e faz uso dos benefícios atrelados à linguagem Python para o desenvolvimento de códigos simples, eficientes e reutilizáveis.\n",
        "\n",
        "A construção desta biblioteca é fundamentada nos pacotes Numpy, Scipy e Matplotlib. Dentre os pacotes oferecidos pelo scikit-learn, podemos destacar tanto o Pandas como o Numpy. Estes pacotes são os pacotes mais utilizados para compor _pipelines_ de dados em projetos de ciência de dados, justamente porque possuem muitas funcionalidades que permitem a preparação dos dados, exploração e análises preditivas de dados com maior agilidade e eficiência."
      ],
      "metadata": {
        "id": "mL8ybeodCSdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Campos de Aplicação\n",
        "\n",
        "Conforme citado acima, a biblioteca scikit-learn foi construída para atender a área de _machine learning_. Desta maneira, dado que existem diversos problemas de negócio com diferentes finalidades, a biblioteca scikil-learn possui uma vasta quantidade de módulos e estimadores que têm como objetivo atender a diferentes necessidades que podemos encontrar em projetos de dados.\n",
        "\n",
        "Abaixo, compartilho alguns módulos e como os mesmos são utilizados dentro da disciplina de _machine learning_:\n",
        "\n",
        "* **Pré-processamento:** Etapa responsável por capturar, preparar e explorar os dados. Em média, tende a utilizar 70% do tempo dentro de um projeto de ciência de dados, sendo a etapa mais custosa. O scikit-learn possui diversas funcionalidades que permitem a captação e exploração dos dados através de estatísticas, como também prepará-los com conversões e transformações necessárias.\n",
        "\n",
        "* **Classificação:** Oferece modelos de classificação para o desenvolvimento de soluções que possam classificar classes ou variáveis dependentes com base em um conjunto de atributos, _features_, instâncias ou variáveis independentes.\n",
        "\n",
        "* **Regressão:** Permite o desenvolvimento de modelos de regressão, onde são produzidos resultados compostos com valores numéricos e contínuos. Através destes modelos, é possível prever o valor de determinados produtos, estimar quantidade de vendas e demais aplicações nesta vertente.\n",
        "\n",
        "* **Clusterização:** Neste campo de aplicação, o sckiti-learn contempla uma vasta quantidade de módulos para solucionar problemas de agrupamento de dados com base no padrão encontrados nas instâncias ou _features_ do conjunto de dados utilizado. Assim, modelos desta natureza, permitem identificar diferentes tipos de clientes que compram um produto da empresa, identificação de padrões de gastos financeiros de usuários em um determinado banco e assim por diante.\n",
        "\n",
        "* **Redução de dimensionalidade:** Técnica contemplada pela biblioteca scikit-learn que proporciona meios de diminuir a quantidade de variáveis de um conjunto de dados sem gerar perdas significativas de eficiência e assertividade nos resultados.\n",
        "\n",
        "* **Ajuste de parâmetros:** Esta técnica permite selecionar, comparar e validar diferentes parâmetros no modelo de forma automatizada. Assim, é possível encontrar a melhor configuração de parâmetros para um modelo, produzindo o resultado mais ótimo possível.\n",
        "\n",
        "Sobre os estimadores, o scikit-learn possui diferentes estimadores, onde uns são mais adequados para diferentes tipos de dados e diferentes problemas do que outros. O fluxograma apresentado abaixo através da Figura 1 - Fluxograma Scikit-Learn, foi projetado para fornecer aos usuários um guia aproximado sobre como abordar problemas em relação a quais estimadores devem ser testados em seus dados.\n",
        "\n",
        "![Figura 1 – Fluxograma Scikit-Learn](https://scikit-learn.org/stable/_static/ml_map.png)  \n",
        "**Figura 1** – Fluxograma Scikit-Learn"
      ],
      "metadata": {
        "id": "-0F56D_WCUR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicação Prática"
      ],
      "metadata": {
        "id": "vugYeyf0DiqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Através do link abaixo, pode ser feito o download do(s) script(s) python criado(s) para exemplificar uma abordagem do uso prático da biblioteca scikit-learn como também o(s) respectivo(s) conjunto(s) de dados utilizados.\n",
        "\n",
        "**Regressão Logística**\n",
        "\n",
        "[Script Python](https://s3-sa-east-1.amazonaws.com/lcpi/a397cfbb-806b-4119-ba03-eefab2fc55e3.ipynb)  \n",
        "[Dataset 1](https://s3-sa-east-1.amazonaws.com/lcpi/e4cdf6c8-c613-480c-8989-f1c3fd053123.csv)  \n",
        "[Dataset 2](https://s3-sa-east-1.amazonaws.com/lcpi/af4b01aa-2cdc-486a-8c41-b4a3cafe3849.csv)  "
      ],
      "metadata": {
        "id": "XH3Wac_HCZw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Let's Code - Regressão Logística\n",
        "\n",
        "#### Carregando as bibliotecas\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# esta linha permite ver os gráficos sem precisar chamar a função \"show()\"\n",
        "%matplotlib inline\n",
        "\n",
        "#### Carregando dos dados\n",
        "df = pd.read_csv('eleicao.csv', sep = ';')\n",
        "plt.scatter(df.DESPESAS, df.SITUACAO)\n",
        "df.describe()\n",
        "\n",
        "#### Verificação da correlação entre variáveis\n",
        "coerr = np.corrcoef(df.DESPESAS, df.SITUACAO)\n",
        "coerr\n",
        "\n",
        "#### Segregando dataset df entre variáveis dependentes e independentes\n",
        "X = df.iloc[:, 2].values\n",
        "X = X[:, np.newaxis]\n",
        "y = df.iloc[:, 1].values\n",
        "\n",
        "#### Criação do modelo\n",
        "modelo = LogisticRegression()\n",
        "modelo.fit(X, y)\n",
        "modelo.coef_\n",
        "modelo.intercept_\n",
        "\n",
        "#### Apresentação dos resultados\n",
        "plt.scatter(X, y)\n",
        "# Geração de novos dados para gerar a função sigmoide\n",
        "X_teste = np.linspace(10, 3000, 100)\n",
        "# Implementação da função sigmoide\n",
        "def model(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "# Geração de previsões (variável r) e visualização dos resultados\n",
        "r = model(X_teste * modelo.coef_ + modelo.intercept_).ravel()\n",
        "plt.plot(X_teste, r, color = 'red')\n",
        "\n",
        "#### Verificando novos candidatos\n",
        "# Carregamento da base de dados com os novos candidatos\n",
        "df_novos_candidatos = pd.read_csv('novoscandidatos.csv', sep = ';')\n",
        "\n",
        "# Mudança dos dados para formato de matriz\n",
        "despesas = df_novos_candidatos.iloc[:, 1].values\n",
        "despesas = despesas.reshape(-1, 1)\n",
        "\n",
        "# Previsões e geração de nova base de dados com os valores originais e as previsões\n",
        "previsoes_teste = model_LR.predict(despesas)\n",
        "df_novos_candidatos = np.column_stack((df_novos_candidatos, previsoes_teste))\n",
        "df_novos_candidatos"
      ],
      "metadata": {
        "id": "_7_Qp3nR_8Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Árvores de Decisão**\n",
        "\n",
        "[Script Python](https://s3-sa-east-1.amazonaws.com/lcpi/5a9066d7-9e1b-4342-b06b-daa265291707.ipynb)  \n",
        "[Dataset 1](https://s3-sa-east-1.amazonaws.com/lcpi/65eb1366-27b4-485b-ada0-b7452c72f065.csv)  "
      ],
      "metadata": {
        "id": "qrE9dVOyCoZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Árvores de Decisão\n",
        "\n",
        "# Carregando as bibliotecas\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Carregando o dataset\n",
        "df_credito = pd.read_csv('credit.csv')\n",
        "df_credito.shape\n",
        "\n",
        "# Apresentando as 5 primeiras linhas do dataset\n",
        "df_credito.head()\n",
        "\n",
        "# Segregando as variáveis previsoras e classe\n",
        "previsores = df_credito.iloc[:,0:20].values\n",
        "classe = df_credito.iloc[:,20].values\n",
        "\n",
        "# Realizando a conversão de atributos categóricos para numéricos de acordo com o respectivo índice\n",
        "labelencoder = LabelEncoder()\n",
        "previsores[:,0] = labelencoder.fit_transform(previsores[:,0])\n",
        "previsores[:,2] = labelencoder.fit_transform(previsores[:,2])\n",
        "previsores[:, 3] = labelencoder.fit_transform(previsores[:, 3])\n",
        "previsores[:, 5] = labelencoder.fit_transform(previsores[:, 5])\n",
        "previsores[:, 6] = labelencoder.fit_transform(previsores[:, 6])\n",
        "previsores[:, 8] = labelencoder.fit_transform(previsores[:, 8])\n",
        "previsores[:, 9] = labelencoder.fit_transform(previsores[:, 9])\n",
        "previsores[:, 11] = labelencoder.fit_transform(previsores[:, 11])\n",
        "previsores[:, 13] = labelencoder.fit_transform(previsores[:, 13])\n",
        "previsores[:, 14] = labelencoder.fit_transform(previsores[:, 14])\n",
        "previsores[:, 16] = labelencoder.fit_transform(previsores[:, 16])\n",
        "previsores[:, 18] = labelencoder.fit_transform(previsores[:, 18])\n",
        "previsores[:, 19] = labelencoder.fit_transform(previsores[:, 19])\n",
        "\n",
        "# Segregando os dados entre treinamento e teste\n",
        "X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(previsores,\n",
        "                                                                  classe,\n",
        "                                                                  test_size = 0.3,\n",
        "                                                                  random_state = 0)\n",
        "\n",
        "# Criação do modelo\n",
        "arvore = DecisionTreeClassifier()\n",
        "\n",
        "# Treinamento do modelo\n",
        "arvore.fit(X_treinamento, y_treinamento)\n",
        "\n",
        "# Obtendo as previsões\n",
        "previsoes = arvore.predict(X_teste)\n",
        "previsoes\n",
        "\n",
        "# Confusion Matrix\n",
        "confusao = confusion_matrix(y_teste, previsoes)\n",
        "confusao\n",
        "\n",
        "# Calculando a taxa de acerto\n",
        "taxa_acerto = accuracy_score(y_teste, previsoes)\n",
        "taxa_acerto\n",
        "\n",
        "# Calculando a taxa de erro\n",
        "taxa_erro = 1 - taxa_acerto\n",
        "taxa_erro"
      ],
      "metadata": {
        "id": "6Fke-anoCddX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNN (K-Nearest Neighbors)**\n",
        "\n",
        "[Script Python](https://s3-sa-east-1.amazonaws.com/lcpi/5bbba88a-fe10-48aa-8307-2d47ddd4a8fd.ipynb)  \n",
        "[Dataset 1](https://s3-sa-east-1.amazonaws.com/lcpi/05a391a0-3767-4bd5-a9c9-6431881ddfc4.csv)  "
      ],
      "metadata": {
        "id": "zQUye_iVCrBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## KNN - K Nearest Neighbor\n",
        "\n",
        "### Importando as bibliotecas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy as scp\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "### Importando o dataset\n",
        "df = pd.read_csv('mushrooms.csv',engine='python', sep=',')\n",
        "\n",
        "### Explorando o dataset\n",
        "df.head()\n",
        "\n",
        "# Informações gerais sobre o dataset\n",
        "df.info()\n",
        "\n",
        "# Produção de estatísticas sobre os dados\n",
        "df.describe()\n",
        "\n",
        "### Verificação de registros nulos no dataset\n",
        "def distribuicao (data):\n",
        "    '''\n",
        "    Esta função exibirá a quantidade de registros únicos para cada coluna\n",
        "    existente no dataset\n",
        "\n",
        "    dataframe -> Histogram\n",
        "    '''\n",
        "    # Calculando valores únicos para cada label: num unique labels\n",
        "    num_unique_labels = data.apply(pd.Series.nunique)\n",
        "\n",
        "    # plotando valores\n",
        "    num_unique_labels.plot( kind='bar')\n",
        "\n",
        "    # Nomeando os eixos\n",
        "    plt.xlabel('Campos')\n",
        "    plt.ylabel('Número de Registros únicos')\n",
        "    plt.title('Distribuição de dados únicos do DataSet')\n",
        "\n",
        "    # Exibindo gráfico\n",
        "    plt.show()\n",
        "\n",
        "distribuicao(df)\n",
        "\n",
        "### Análise de distribuição dos dados da classe Y (Venenoso = p, Comestível = e)\n",
        "e = pd.value_counts(df['class']) [0]\n",
        "p = pd.value_counts(df['class']) [1]\n",
        "\n",
        "tam = len(df)\n",
        "\n",
        "print('Cogumelos Comestiveis: ',e)\n",
        "print('Cogumelos Venenosos: ',p )\n",
        "\n",
        "pie = pd.DataFrame([['Comestivel',e],['Venenoso',p]],columns=['Tipo' , 'Quantidade'])\n",
        "\n",
        "\n",
        "def pie_chart(data,col1,col2,title):\n",
        "    labels = {'Comestivel':0,'Venenoso':1}\n",
        "    sizes = data[col2]\n",
        "    colors = ['#e5ffcc', '#ffb266']\n",
        "\n",
        "    plt.pie(sizes, labels=labels, colors=colors,\n",
        "                autopct='%1.1f%%', shadow=True, startangle=140, labeldistance =1.2)\n",
        "    plt.title( title )\n",
        "\n",
        "    plt.axis('equal')\n",
        "    plt.show()\n",
        "\n",
        "pie_chart(pie,'Tipo' , 'Quantidade','Distribuição Percentual Classes de Cogumelos')\n",
        "\n",
        "\n",
        "plt.bar(pie.Tipo,pie.Quantidade, color = ['#e5ffcc', '#ffb266'])\n",
        "plt.title(\"Distribuição das Classes de Cogumelos\")\n",
        "plt.xlabel(\"Tipo de Cogumelo\")\n",
        "plt.ylabel('Quantidade de Registros')\n",
        "plt.show()\n",
        "\n",
        "### Split do conjunto de dados\n",
        "# X = colunas de informação, variáveis independentes\n",
        "X = df.drop('class', axis=1)\n",
        "\n",
        "# y = Variável dependente, a qual será utilizada para classificar os dados\n",
        "y = df['class']\n",
        "\n",
        "# Verificando se X está com a coluna class\n",
        "X.head()\n",
        "\n",
        "### Aplicação da técnica One Hot Encoder\n",
        "\n",
        "#### Transformar as labels em números.\n",
        "#### O OneHotEncoder gera novas colunas com valor 0 ou 1, em que 1 representa a ocorrência daquela característica e 0 a não ocorrência.\n",
        "\n",
        "#Importando o encoder para transformar as labels em chaves numéricas\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "Oht_enc = OneHotEncoder()\n",
        "X = pd.DataFrame(Oht_enc.fit_transform(X).A)\n",
        "X.shape\n",
        "\n",
        "### Train Test Split\n",
        "\n",
        "#### Nesta fase separamos o conjunto de dados em Treinamento e Teste, definindo o percentual que utilizaremos para teste e para treino do modelo\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.3)\n",
        "\n",
        "### Feature Scaling\n",
        "\n",
        "#### Etapa importante que irá reduzir a escala numérica das colunas, para que todas estejam dentro de uma mesma escala de valor, lembrando que na matemática os números são infinitos dentro de suas escalas, podendo serem representados então em diversas escalas diferentes. Se houver medidas com escalas de valor muito diferentes, a distância calculada pelo algoritmo será influenciada podendo gerar resultados errôneos.\n",
        "\n",
        "#Importing librarie\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "### Creating KNN Model\n",
        "\n",
        "#### Agora iremos aplicar nossos dados ao algoritmo KNN\n",
        "\n",
        "#Importando o modelo KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Definindo o valor de vizinhos\n",
        "classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "#Treinando o modelo, com dados de treinamento\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "#### Prevendo os valores de Y para os dados de teste (X_test)\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "### Avaliando o Algoritmo\n",
        "##### Analisando e validando os resultados obtidos\n",
        "\n",
        "# Importando métricas para validação do modelo\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Imprimindo a matriz confusa\n",
        "print(\"Matriz Confusa: \")\n",
        "print(confusion_matrix(y_test, y_pred), \"\\n\")\n",
        "\n",
        "# Imprimindo o relatório de classificação\n",
        "print(\"Relatório de classificação: \\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# Imprimindo o quão acurado foi o modelo\n",
        "print('Acurácia do modelo: ' , accuracy_score(y_test, y_pred))\n",
        "\n",
        "### Loop para gerar testes com diferentes valores de vizinho (K)\n",
        "error = []\n",
        "\n",
        "# Calculating error for K values between 1 and 40\n",
        "for i in range(1, 10):\n",
        "    knn = KNeighborsClassifier(n_neighbors=i)\n",
        "    knn.fit(X_train, y_train)\n",
        "    pred_i = knn.predict(X_test)\n",
        "    error.append(np.mean(pred_i != y_test))\n",
        "\n",
        "### Comparando o Error Rate gerado de valores K diferentes\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, 10), error, color='red', linestyle='dashed', marker='o',\n",
        "         markerfacecolor='blue', markersize=10)\n",
        "plt.title('Error Rate K Value')\n",
        "plt.xlabel('K Value')\n",
        "plt.ylabel('Mean Error')\n",
        "\n",
        "### Aplicando melhor parâmetro para K encontrado\n",
        "# Treinando o modelo KNN com o melhor parâmetro para K\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=1)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Aplicando os valores de teste novamente\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Importando métricas para validação do modelo\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Imprimindo a matriz confusa\n",
        "print(\"Matriz Confusa: \")\n",
        "print(confusion_matrix(y_test, y_pred), \"\\n\")\n",
        "\n",
        "# Imprimindo o relatório de classificação\n",
        "print(\"Relatório de classificação: \\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# Imprimindo o quão acurado foi o modelo\n",
        "print('Acurácia do modelo: ' , accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "-9hbvAbECr4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes**\n",
        "\n",
        "[Script Python](https://s3-sa-east-1.amazonaws.com/lcpi/6215b396-0597-48fe-ad75-1af890da2382.ipynb)  \n",
        "[Dataset 1](https://s3-sa-east-1.amazonaws.com/lcpi/dacaec02-38a5-4ad5-ad05-ee6a11f8a833.csv)  \n",
        "[Dataset 2](https://s3-sa-east-1.amazonaws.com/lcpi/024de15e-c69b-4e3d-b39a-a0a69239b0a1.csv)  "
      ],
      "metadata": {
        "id": "Kc2kSSdrDAGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Naive Bayes\n",
        "\n",
        "#### Carregando as bibliotecas\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "\n",
        "#### Carregando conjunto de dados\n",
        "credito = pd.read_csv('credit.csv')\n",
        "credito.shape\n",
        "\n",
        "# Apresentando as 5 primeiras linhas\n",
        "credito.head()\n",
        "\n",
        "#### Alterando para um formato de matriz\n",
        "previsores = credito.iloc[:,0:20].values\n",
        "classe = credito.iloc[:,20].values\n",
        "\n",
        "#### Transformação dos atributos categóricos em atributos numéricos\n",
        "labelencoder1 = LabelEncoder()\n",
        "previsores[:,0] = labelencoder1.fit_transform(previsores[:,0])\n",
        "\n",
        "labelencoder2 = LabelEncoder()\n",
        "previsores[:,2] = labelencoder2.fit_transform(previsores[:,2])\n",
        "\n",
        "labelencoder3 = LabelEncoder()\n",
        "previsores[:, 3] = labelencoder3.fit_transform(previsores[:, 3])\n",
        "\n",
        "labelencoder4 = LabelEncoder()\n",
        "previsores[:, 5] = labelencoder4.fit_transform(previsores[:, 5])\n",
        "\n",
        "labelencoder5 = LabelEncoder()\n",
        "previsores[:, 6] = labelencoder5.fit_transform(previsores[:, 6])\n",
        "\n",
        "labelencoder6 = LabelEncoder()\n",
        "previsores[:, 8] = labelencoder6.fit_transform(previsores[:, 8])\n",
        "\n",
        "labelencoder7 = LabelEncoder()\n",
        "previsores[:, 9] = labelencoder7.fit_transform(previsores[:, 9])\n",
        "\n",
        "labelencoder8 = LabelEncoder()\n",
        "previsores[:, 11] = labelencoder8.fit_transform(previsores[:, 11])\n",
        "\n",
        "labelencoder9 = LabelEncoder()\n",
        "previsores[:, 13] = labelencoder9.fit_transform(previsores[:, 13])\n",
        "\n",
        "labelencoder10 = LabelEncoder()\n",
        "previsores[:, 14] = labelencoder10.fit_transform(previsores[:, 14])\n",
        "\n",
        "labelencoder11 = LabelEncoder()\n",
        "previsores[:, 16] = labelencoder11.fit_transform(previsores[:, 16])\n",
        "\n",
        "labelencoder12 = LabelEncoder()\n",
        "previsores[:, 18] = labelencoder12.fit_transform(previsores[:, 18])\n",
        "\n",
        "labelencoder13 = LabelEncoder()\n",
        "previsores[:, 19] = labelencoder13.fit_transform(previsores[:, 19])\n",
        "\n",
        "#### Segregando o conjunto de dados entre dados de treino e teste\n",
        "X_treinamento, X_teste, y_treinamento, y_teste = train_test_split(previsores,\n",
        "                                                                  classe,\n",
        "                                                                  test_size = 0.3,\n",
        "                                                                  random_state = 0)\n",
        "X_teste\n",
        "\n",
        "#### Criando o modelo\n",
        "# naive_bayes = GaussianNB()\n",
        "naive_bayes.fit(X_treinamento, y_treinamento)\n",
        "\n",
        "#### Geração de previsões\n",
        "previsoes = naive_bayes.predict(X_teste)\n",
        "previsoes\n",
        "\n",
        "#### Matriz de confusão\n",
        "confusao = confusion_matrix(y_teste, previsoes)\n",
        "confusao\n",
        "\n",
        "# Calculando a taxa de acerto e de erro\n",
        "taxa_acerto = accuracy_score(y_teste, previsoes)\n",
        "taxa_erro = 1 - taxa_acerto\n",
        "taxa_acerto\n",
        "\n",
        "# Gerando gráfico da matrix de confusão\n",
        "v = ConfusionMatrix(GaussianNB())\n",
        "v.fit(X_treinamento, y_treinamento)\n",
        "v.score(X_teste, y_teste)\n",
        "v.poof()\n",
        "\n",
        "#### Previsão de novos registros\n",
        "\n",
        "# Carregando novos dados\n",
        "novo_credito = pd.read_csv('novo_credit.csv')\n",
        "novo_credito.shape\n",
        "\n",
        "#### Transformação dos atributos categóricos em atributos numéricos\n",
        "novo_credito = novo_credito.iloc[:,0:20].values\n",
        "novo_credito[:,0] = labelencoder1.transform(novo_credito[:,0])\n",
        "novo_credito[:, 2] = labelencoder2.transform(novo_credito[:, 2])\n",
        "novo_credito[:, 3] = labelencoder3.transform(novo_credito[:, 3])\n",
        "novo_credito[:, 5] = labelencoder4.transform(novo_credito[:, 5])\n",
        "novo_credito[:, 6] = labelencoder5.transform(novo_credito[:, 6])\n",
        "novo_credito[:, 8] = labelencoder6.transform(novo_credito[:, 8])\n",
        "novo_credito[:, 9] = labelencoder7.transform(novo_credito[:, 9])\n",
        "novo_credito[:, 11] = labelencoder8.transform(novo_credito[:, 11])\n",
        "novo_credito[:, 13] = labelencoder9.transform(novo_credito[:, 13])\n",
        "novo_credito[:, 14] = labelencoder10.transform(novo_credito[:, 14])\n",
        "novo_credito[:, 16] = labelencoder11.transform(novo_credito[:, 16])\n",
        "novo_credito[:, 18] = labelencoder12.transform(novo_credito[:, 18])\n",
        "novo_credito[:, 19] = labelencoder13.transform(novo_credito[:, 19])\n",
        "\n",
        "#### Resultado da previsão\n",
        "naive_bayes.predict(novo_credito)"
      ],
      "metadata": {
        "id": "R6xDW4VTC481"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Otimização de Hiperparâmetros**\n",
        "\n",
        "[Script Python](https://s3-sa-east-1.amazonaws.com/lcpi/1f9a4fd4-3521-47dd-b851-724ae85d6af4.ipynb)  "
      ],
      "metadata": {
        "id": "JbY9R4ixDEKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Otimização de Hiperparâmetros\n",
        "\n",
        "# Definindo variável seed\n",
        "SEED = 123456\n",
        "\n",
        "#### Importando a base dados da própria biblioteca Sklearn\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Carregando dataset\n",
        "df = load_breast_cancer()\n",
        "\n",
        "# Importando biblioteca pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Formatando dataset em um DataFrame e apresentado a 5 primeiras linhas do mesmo\n",
        "df_feature = pd.DataFrame(data=df['data'], columns=df['feature_names'])\n",
        "df_feature.head()\n",
        "\n",
        "# Definindo a variável target\n",
        "df_targets = pd.Series(data=df['target'], name='benign')\n",
        "\n",
        "# Apresentando os valores únicos da variável target\n",
        "df_targets.unique()\n",
        "\n",
        "# Atribuindo features na variável X e target na variável y\n",
        "X = df_feature\n",
        "y = df_targets\n",
        "\n",
        "#### Carregando o algoritmo de Árvores de Decisão para ser o algoritmo na qual vamos aplicar a otimização\n",
        "# Carregando as bibliotecas\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_validate\n",
        "import numpy as np\n",
        "\n",
        "# Criando o modelo\n",
        "modelo_tree = DecisionTreeClassifier()\n",
        "\n",
        "# Aplicando a técnica de cross validade\n",
        "results = cross_validate(modelo_tree, X, y, cv=5,\n",
        "               scoring=('accuracy'),\n",
        "               return_train_score=True)\n",
        "print(f\"mean_train_score {np.mean(results['train_score']):.2f}\")\n",
        "print(f\"mean_test_score {np.mean(results['test_score']):.2f}\")\n",
        "\n",
        "#### Abordando o uso da otimização através do Grid Search\n",
        "# Carregando a biblioteca GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Definindo parâmetros\n",
        "relacao_parametros = {\n",
        "  \"max_depth\" : [3, 5],\n",
        "  \"min_samples_split\" : [32, 64, 128],\n",
        "  \"min_samples_leaf\" : [32, 64, 128],\n",
        "  \"criterion\" : [\"gini\", \"entropy\"]\n",
        "}\n",
        "\n",
        "# Criando modelo\n",
        "modelo_tree = DecisionTreeClassifier()\n",
        "\n",
        "# Aplicando o algoritmo com parâmetros definidos anteriormente\n",
        "clf = GridSearchCV(modelo_tree, relacao_parametros, cv=5, return_train_score=True, scoring='accuracy')\n",
        "\n",
        "# Treinando o modelo\n",
        "search = clf.fit(X, y)\n",
        "\n",
        "# Capturando os resultados e os índices dos melhores parâmetros\n",
        "results_GridSearchCV = search.cv_results_\n",
        "indice_melhores_parametros = search.best_index_\n",
        "\n",
        "# Apresentando a média de score de treino e teste produzida\n",
        "print(f\"mean_train_score {results_GridSearchCV['mean_train_score'][indice_melhores_parametros]:.2f}\")\n",
        "print(f\"mean_test_score {results_GridSearchCV['mean_test_score'][indice_melhores_parametros]:.2f}\")\n",
        "\n",
        "# Apresentação dos parâmetros\n",
        "results_GridSearchCV['params'][indice_melhores_parametros]\n",
        "\n",
        "#### Abordando o uso da otimização através do Random Search\n",
        "# Carregando as variáveis\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Definindo relação de parâmetros\n",
        "relacao_parametros_2 = {\n",
        "    \"max_depth\" : randint(1, 10),\n",
        "    \"min_samples_split\" : randint(32, 129),\n",
        "    \"min_samples_leaf\" : randint(32, 129),\n",
        "    \"criterion\" : [\"gini\", \"entropy\"]\n",
        "}\n",
        "\n",
        "# Criação do modelo\n",
        "modelo_tree = DecisionTreeClassifier()\n",
        "\n",
        "clf = RandomizedSearchCV(modelo_tree, relacao_parametros_2, random_state=SEED, cv=5, return_train_score=True, n_iter=10, scoring='accuracy')\n",
        "search = clf.fit(X, y)\n",
        "results_RandomizedSearchCV = search.cv_results_\n",
        "indice_melhores_parametros = search.best_index_\n",
        "\n",
        "# Apresentando a média de score de treino e teste produzida\n",
        "print(f\"mean_train_score {results_RandomizedSearchCV['mean_train_score'][indice_melhores_parametros]:.2f}\")\n",
        "print(f\"mean_test_score {results_RandomizedSearchCV['mean_test_score'][indice_melhores_parametros]:.2f}\")\n",
        "\n",
        "# Apresentação dos parâmetros\n",
        "results_RandomizedSearchCV['params'][indice_melhores_parametros]"
      ],
      "metadata": {
        "id": "-ZaC0GqsDFtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Materiais complementares\n",
        "\n",
        "* Programação Dinâmica at Youtube: [Primeiros passos com Scikit-Learn | Machine Learning #04](https://www.youtube.com/watch?v=39HBlzFV9vk)\n",
        "* Let’s Code channel at Youtube: [Machine Learning além das previsões](https://www.youtube.com/watch?v=aif9hcj6T5w)\n",
        "* Let’s Code channel at Youtube: [Como instalar bibliotecas no Python](https://www.youtube.com/watch?v=cDqMbI02hRs)\n",
        "* [Scikit-Learn: _Machine Learning_ in Python — scikit-learn 1.0.2 documentation.](https://scikit-learn.org/stable/). Acessado 23 de fevereiro de 2022.\n",
        "\n",
        "\n",
        "## Referências\n",
        "\n",
        "* [Scikit-Learn: _Machine Learning_ in Python — scikit-learn 1.0.2 documentation.](https://scikit-learn.org/stable/). Acessado 23 de fevereiro de 2022."
      ],
      "metadata": {
        "id": "kgV-XPP6DLoF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9CPlIunjDL3s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
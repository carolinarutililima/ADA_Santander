{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Otimização de Hiperparâmetros"
      ],
      "metadata": {
        "id": "oefLrGvL5duA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conceito\n",
        "\n",
        "As técnicas de otimização de hiperparâmetros são peças fundamentais dentro de um projeto de dados. É através delas que conseguimos extrair o melhor desempenho do modelo e conseguimos elevar o nível da solução de dados que estamos construindo.\n",
        "\n",
        "É importante salientar e clarear as características entre parâmetros e hiperparâmetros dentro dos processos de modelagem que devido utilizarem a mesma palavra, são facilmente confundidos e definidos como a mesma coisa de maneira errada.\n",
        "\n",
        "Os parâmetros, por sua vez, são ajustados diretamente por processos de aprendizado dos algoritmos e possuem influência direta no desempenho deles. As fronteiras de vizinhos no KNN, os pesos definidos dentro de redes neurais, coeficientes de regressão linear, todos estes são exemplos de parâmetros que se ajustam no momento do treinamento do modelo com base em um conjunto de dados.\n",
        "\n",
        "Por outro lado, os hiperparâmetros são variáveis do algoritmo que são definidas antes do treinamento. Os hiperparâmetros são características construtivas do algoritmo. Neste caso, utilizando os exemplos citados para exemplificar os parâmetros, nos hiperparâmetros consideramos a quantidade de vizinhos definido como K no KNN, a quantidade de camadas que vamos utilizar nas redes neurais e as métricas de desempenho de regressões lineares. Os hiperparâmetros têm forte influência no desempenho dos modelos e por isso, a escolha e o valor agregado aos mesmos devem ser definidos de maneira criteriosa.\n",
        "\n",
        "Para produzir a otimização de um determinado _hiperparâmetro_ é necessário realizar o treinamento do modelo diversas vezes. Somente assim, podemos realizar os testes e validar se o desempenho do nosso modelo obteve melhorias com base nos hiperparâmetros e valores definidos para eles. Estes processos de treinar várias vezes o modelo e posteriormente testar os hiperparâmetros do algoritmo treinado possui alto custo computacional e tende a ser um pouco demorado.\n",
        "\n",
        "A fim de resolver esses gargalos e problemas no processo de melhoria da modelagem, os algoritmos de otimização foram criados para auxiliar-nos no desenvolvimento de processos de definição de hiperparâmetros e seus respectivos valores que são tão importantes em projetos de dados."
      ],
      "metadata": {
        "id": "56mvZL8u5gDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principais Algoritmos de Otimização\n",
        "\n",
        "Abaixo, são compartilhados os principais algoritmos de otimização de hiperparâmetros:\n",
        "\n",
        "* _Grid Search_: Possivelmente o caso mais ingênuo e mais simples. Este algoritmo de busca recebe um conjunto de valores de um ou mais hiperparâmetros e testa todas as combinações possíveis. Desta maneira, ele tabula o desempenho de cada configuração e ao final, indica a melhor opção de configuração para ser utilizada pelo algoritmo.\n",
        "\n",
        "* _Random Search_: O _Random Search_ é bastante semelhante ao _Grid Search_. A principal diferença entre eles é que ao invés de testar todas as combinações possíveis de configuração entre hiperparâmetros e valores que ele pode assumir, o _Random Search_ estabelece o teste de combinações aleatórias de acordo com um número especificado de amostras. Quando a quantidade de dados envolvidos e a quantidade de parâmetros for muito grande, este algoritmo se torna uma melhor opção do que _Grid Search_.\n",
        "\n",
        "* _Bayes Search_: Utilizando a busca bayesiana, o algoritmo _Bayes Search_ estima a melhor combinação ou configuração de hiperparâmetros fundamentados nas distribuições criadas de combinações testadas anteriormente. Internamente, este algoritmo encontra as regiões de menor confiança na distribuição gerada e dentro destas regiões ele verifica qual das mesmas contém um valor mais elevado para o desempenho do algoritmo. Desta maneira, a cada iteração da busca bayesiana, as configurações ou combinações de melhor desempenho são aplicadas aos hiperparâmetros e por sua vez, a distribuição do desempenho do modelo é frequentemente atualizada. Em termos práticos, o ganho de desempenho proporcionado por este algoritmo é muito superior quando comparado com o _Grid Search_ e o _Random Search_. Assim, ele é um algoritmo indicado para tarefas de otimização mais complexas. No entanto, é válido destacar que o _Bayes Search_ consome mais tempo do que os algoritmos _Grid Search_ e _Random Search_."
      ],
      "metadata": {
        "id": "3f6Y7Q105iGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Campos de Aplicação\n",
        "\n",
        "O uso da técnica de otimização de hiperparâmetros pode e deve ser aplicado em qualquer projeto de dados. Independente do problema que esteja sendo tratado, é importante explorar os algoritmos de _machine learning_ como também os hiperparâmetros que os constituem para que possam existir meios de realizar o _tuning_ ou melhorias do desempenho do modelo sendo testado e assim, alterando o seu comportamento padrão.\n",
        "\n",
        "Diversos algoritmos aplicados a problemas de aprendizado supervisionado, não supervisionado, por reforço e _deep learning_ possuem tanto parâmetros para realização da modelagem adotando um comportamento padrão como também hiperparâmetros que podem ser ajustados para adequar o desempenho dele ao seu problema específico.\n",
        "\n",
        "É importante destacar que para modificações em poucos parâmetros esse processo pode ser feito manualmente. Porém, à medida que a quantidade de hiperparâmetros aumenta, a quantidade de testes que temos que fazer aumentará exponencialmente e o uso das técnicas exploradas nesta sessão serão uteis para proporcionar o melhor uso do tempo de modelagem como também produzir o melhor desempenho possível dos algoritmos utilizados."
      ],
      "metadata": {
        "id": "HiKOklsy5jxk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicação Prática\n",
        "\n",
        "Através do link abaixo, pode ser feito o download do(s) script(s) python criado(s) para exemplificar uma abordagem do uso prático da otimização de hiperparâmetros como também o(s) respectivo(s) conjunto(s) de dados utilizados.\n",
        "\n",
        "[Script Python](https://s3-sa-east-1.amazonaws.com/lcpi/1f9a4fd4-3521-47dd-b851-724ae85d6af4.ipynb)"
      ],
      "metadata": {
        "id": "eouZLVKR5mHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Otimização de Hiperparâmetros\n",
        "\n",
        "# Definindo variável seed\n",
        "SEED = 123456\n",
        "\n",
        "#### Importando a base dados da própria biblioteca Sklearn\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Carregando dataset\n",
        "df = load_breast_cancer()\n",
        "\n",
        "# Importando biblioteca pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Formatando dataset em um DataFrame e apresentado a 5 primeiras linhas do mesmo\n",
        "df_feature = pd.DataFrame(data=df['data'], columns=df['feature_names'])\n",
        "df_feature.head()\n",
        "\n",
        "# Definindo a variável target\n",
        "df_targets = pd.Series(data=df['target'], name='benign')\n",
        "\n",
        "# Apresentando os valores únicos da variável target\n",
        "df_targets.unique()\n",
        "\n",
        "# Atribuindo features na variável X e target na variável y\n",
        "X = df_feature\n",
        "y = df_targets\n",
        "\n",
        "#### Carregando o algoritmo de Árvores de Decisão para ser o algoritmo na qual vamos aplicar a otimização\n",
        "# Carregando as bibliotecas\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_validate\n",
        "import numpy as np\n",
        "\n",
        "# Criando o modelo\n",
        "modelo_tree = DecisionTreeClassifier()\n",
        "\n",
        "# Aplicando a técnica de cross validade\n",
        "results = cross_validate(modelo_tree, X, y, cv=5,\n",
        "               scoring=('accuracy'),\n",
        "               return_train_score=True)\n",
        "print(f\"mean_train_score {np.mean(results['train_score']):.2f}\")\n",
        "print(f\"mean_test_score {np.mean(results['test_score']):.2f}\")\n",
        "\n",
        "#### Abordando o uso da otimização através do Grid Search\n",
        "# Carregando a biblioteca GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Definindo parâmetros\n",
        "relacao_parametros = {\n",
        "  \"max_depth\" : [3, 5],\n",
        "  \"min_samples_split\" : [32, 64, 128],\n",
        "  \"min_samples_leaf\" : [32, 64, 128],\n",
        "  \"criterion\" : [\"gini\", \"entropy\"]\n",
        "}\n",
        "\n",
        "# Criando modelo\n",
        "modelo_tree = DecisionTreeClassifier()\n",
        "\n",
        "# Aplicando o algoritmo com parâmetros definidos anteriormente\n",
        "clf = GridSearchCV(modelo_tree, relacao_parametros, cv=5, return_train_score=True, scoring='accuracy')\n",
        "\n",
        "# Treinando o modelo\n",
        "search = clf.fit(X, y)\n",
        "\n",
        "# Capturando os resultados e os índices dos melhores parâmetros\n",
        "results_GridSearchCV = search.cv_results_\n",
        "indice_melhores_parametros = search.best_index_\n",
        "\n",
        "# Apresentando a média de score de treino e teste produzida\n",
        "print(f\"mean_train_score {results_GridSearchCV['mean_train_score'][indice_melhores_parametros]:.2f}\")\n",
        "print(f\"mean_test_score {results_GridSearchCV['mean_test_score'][indice_melhores_parametros]:.2f}\")\n",
        "\n",
        "# Apresentação dos parâmetros\n",
        "results_GridSearchCV['params'][indice_melhores_parametros]\n",
        "\n",
        "#### Abordando o uso da otimização através do Random Search\n",
        "# Carregando as variáveis\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Definindo relação de parâmetros\n",
        "relacao_parametros_2 = {\n",
        "    \"max_depth\" : randint(1, 10),\n",
        "    \"min_samples_split\" : randint(32, 129),\n",
        "    \"min_samples_leaf\" : randint(32, 129),\n",
        "    \"criterion\" : [\"gini\", \"entropy\"]\n",
        "}\n",
        "\n",
        "# Criação do modelo\n",
        "modelo_tree = DecisionTreeClassifier()\n",
        "\n",
        "clf = RandomizedSearchCV(modelo_tree, relacao_parametros_2, random_state=SEED, cv=5, return_train_score=True, n_iter=10, scoring='accuracy')\n",
        "search = clf.fit(X, y)\n",
        "results_RandomizedSearchCV = search.cv_results_\n",
        "indice_melhores_parametros = search.best_index_\n",
        "\n",
        "# Apresentando a média de score de treino e teste produzida\n",
        "print(f\"mean_train_score {results_RandomizedSearchCV['mean_train_score'][indice_melhores_parametros]:.2f}\")\n",
        "print(f\"mean_test_score {results_RandomizedSearchCV['mean_test_score'][indice_melhores_parametros]:.2f}\")\n",
        "\n",
        "# Apresentação dos parâmetros\n",
        "results_RandomizedSearchCV['params'][indice_melhores_parametros]"
      ],
      "metadata": {
        "id": "uN7h4R_wDUsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Materiais complementares\n",
        "\n",
        "* Abhishek Thakur channel at Youtube: [Hyperparameter Optimization: This Tutorial Is All You Need](https://www.youtube.com/watch?v=5nYqK-HaoKY)\n",
        "* BERGSTRA, James et al. Algorithms for hyper-parameter optimization. Advances in neural information processing systems, v. 24, 2011.\n",
        "* BARTMANN, Nico et al. Applied Predictive Process Monitoring and Hyper Parameter Optimization in Camunda. In: International Conference on Advanced Information Systems Engineering. Springer, Cham, 2021. p. 129-136.\n",
        "* SUI, Guoxin; YU, Yong. Bayesian contextual bandits for hyper parameter optimization. IEEE Access, v. 8, p. 42971-42979, 2020.\n",
        "* GUPTA, Jayesh. Exploration Study of Ensembled Object Detection models and Hyper Parameter Optimization. Exploration Study of Ensembled Object Detection models and Hyper Parameter Optimization, [S. l.], p. 2-9, 2 nov. 2021.\n",
        "* OTIMIZACAO EVOLUTIVA DE HIPERPARAMETROS PARA MODELOS DE SERIES TEMPORAIS NEBULOSAS. Anais do 14º Simpósio Brasileiro de Automação Inteligente, [S. l.], p. 2-7, 1 out. 2019.\n",
        "* SMITH, Michael R.; MARTINEZ, Tony; GIRAUD-CARRIER, Christophe. The potential benefits of filtering versus hyper-parameter optimization. arXiv preprint arXiv:1403.3342, 2014.\n",
        "\n",
        "\n",
        "## Referências\n",
        "\n",
        "* BERGSTRA, James et al. Algorithms for hyper-parameter optimization. Advances in neural information processing systems, v. 24, 2011.\n",
        "* BARTMANN, Nico et al. Applied Predictive Process Monitoring and Hyper Parameter Optimization in Camunda. In: International Conference on Advanced Information Systems Engineering. Springer, Cham, 2021. p. 129-136.\n",
        "* SUI, Guoxin; YU, Yong. Bayesian contextual bandits for hyper parameter optimization. IEEE Access, v. 8, p. 42971-42979, 2020.\n",
        "* GUPTA, Jayesh. Exploration Study of Ensembled Object Detection models and Hyper Parameter Optimization. Exploration Study of Ensembled Object Detection models and Hyper Parameter Optimization, [S. l.], p. 2-9, 2 nov. 2021.\n",
        "* OTIMIZACAO EVOLUTIVA DE HIPERPARAMETROS PARA MODELOS DE SERIES TEMPORAIS NEBULOSAS. Anais do 14º Simpósio Brasileiro de Automação Inteligente, [S. l.], p. 2-7, 1 out. 2019.\n",
        "* SMITH, Michael R.; MARTINEZ, Tony; GIRAUD-CARRIER, Christophe. The potential benefits of filtering versus hyper-parameter optimization. arXiv preprint arXiv:1403.3342, 2014."
      ],
      "metadata": {
        "id": "yDOl0F4E5qW_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_nSTr7K5ru0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}